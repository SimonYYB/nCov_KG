{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问题分类算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一、问题描述"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "知识图谱的自动问答系统，问题分类属于中文短文本分类，问题描述如下：\n",
    "输入一个中文问句，把问句分成下面四种类型：\n",
    "1）疫情\n",
    "2）实体\n",
    "3）知识\n",
    "4）未知\n",
    "\n",
    "1）疫情：是指某实体（国家、地区、省市、机构）的疫情情况（确诊、治愈、死亡人数），或者防疫措施、政策等。\n",
    "广元的疫情情况如何？ \n",
    "美国疫情情况如何？\n",
    "四川有哪些防控措施？\n",
    "四川信息职业技术学院有哪些防控措施？\n",
    "四川信息职业技术学院的防疫政策？\n",
    "\n",
    "2）实体：是指查询某实体（国家、地区、省市、机构、人）的情况\n",
    "钟南山是谁？\n",
    "美国是什么？\n",
    "尼日利亚是哪里？\n",
    "\n",
    "3）知识：特指新冠肺炎相关的知识，其他知识不管。所以主体包括（新冠、新型冠状、新型肺炎等等）。\n",
    "新冠肺炎来源？\n",
    "新冠肺炎是什么？\n",
    "怎么预防新型冠状肺炎？\n",
    "哪些药物可以治新冠肺炎？\n",
    "新冠肺炎有特效药吗？\n",
    "新冠肺炎有什么药？\n",
    "新冠病毒有疫苗吗？\n",
    "\n",
    "4）未知：无法分类的问题都归入到未知\n",
    "今天星期几？\n",
    "刘德华是谁？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二、经典机器学习方法以及深度学习方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "经典的机器学习方法采用获取tf-idf文本特征，分别喂入logistic regression分类器和随机森林分类器的思路，并对两种方法做性能对比。\n",
    "基于深度学习的文本分类，这里主要使用卷积神经网络以及循环神经网络进行中文文本分类。\n",
    "CNN+RNN模型：\n",
    "输入层：汉字Embedding，填充为不超过32个汉字。\n",
    "CNN层："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 三、数据集准备"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据集比较困难，现在想到的办法是从微博关键词搜索结果中找到所有疑问句，然后对疑问句进行人工打label，再进行数据增强，获得训练集和测试集。\n",
    "\n",
    "1）生成疑问句：先用pyltp完成分句操作，然后查找问号结尾的句子。对句子进行数据清洗（去掉特殊符号）\n",
    "\n",
    "2）人工标注：将所有疑问句放到excel表格中，进行人工标注。大概需要标注500-1000个，作为种子问题。\n",
    "\n",
    "3）数据增强：使用neo4j数据库中的实体和关系，对疑问句中的人名、地名、政府机构、国家、城市等进行随机替换，基本可以生成任意多个问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "11473\n"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from pyltp import SentenceSplitter # 中文分句\n",
    "\n",
    "def clean_text(txt):\n",
    "    #txt = re.sub('[0-9a-zA-Z]+', '', txt)\n",
    "    txt = txt.replace('\\n','')\n",
    "    txt = txt.replace('\\t','')\n",
    "    txt = txt.replace('【','')\n",
    "    txt = txt.replace('】','')\n",
    "    txt = txt.replace('/','')\n",
    "    txt = txt.replace('\\\\','')\n",
    "    txt = txt.replace('?','？')\n",
    "    txt = txt.replace(',','，')\n",
    "    txt = txt.replace(';','；')\n",
    "    txt = txt.replace(\"'\",'')\n",
    "    txt = txt.replace('\"','')\n",
    "    txt = txt.replace(' ','')\n",
    "    txt = txt.replace('　','')\n",
    "    return txt\n",
    "\n",
    "def find_questions_in_file( txt_file ):\n",
    "    with open(txt_file,'r',encoding='utf8') as f:\n",
    "        txt = f.read()\n",
    "        txt = clean_text(txt)\n",
    "        sents = SentenceSplitter.split(txt)# 分句\n",
    "        #print(len(sents))\n",
    "        qlist = list()\n",
    "        for s in sents:\n",
    "            if s[len(s)-1]=='？' and len(s)>3 and len(s)<=32:\n",
    "                qlist.append(s+'\\n')\n",
    "    return qlist\n",
    "\n",
    "def save_questions( qlist, outfile ):\n",
    "    with open(outfile,'w',encoding='utf8') as f:\n",
    "        f.writelines(qlist)\n",
    "\n",
    "rootdir = './dataset/weibo'\n",
    "all_files = os.listdir(rootdir) #列出文件夹下所有的目录与文件\n",
    "questions = list()\n",
    "for i in range(0,len(all_files)):\n",
    "    path = os.path.join(rootdir,all_files[i])\n",
    "    if os.path.isfile(path):\n",
    "        questions += find_questions_in_file(path)\n",
    "\n",
    "save_questions(questions,'./dataset/questions.txt')\n",
    "print(len(questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#再对questions.txt进行二次清洗\n",
    "import csv\n",
    "\n",
    "KEY_WORDS=list(['ECMO','措施','政策','策略','监管','案','例','诊','院','治','愈','死','去世','逝世',\n",
    "    '多少','人','是','哪位','什么','哪里','怎么','新冠','新型冠状','来源','药物','特效药',\n",
    "    '治疗','症','预','护','设备','非典','SARS','MERS','CDC','病毒','炎','复工','健康码','封城','感染',\n",
    "    '传染','呼吸','毒','传播','检疫','H1N1','医','药','病','疾','疫','冠','肺','省','市','国'])\n",
    "lines = list()\n",
    "qdict = dict()\n",
    "with open('./dataset/questions.txt','r',encoding='utf8') as f:\n",
    "    lines = f.readlines()\n",
    "    out = list()\n",
    "    for ll in lines:\n",
    "        for w in KEY_WORDS:\n",
    "            if ll.find(w)>=0:\n",
    "                if ll in qdict:\n",
    "                    continue\n",
    "                else: \n",
    "                    qdict[ll] = 1\n",
    "                    out.append(ll)#+'\\n')\n",
    "save_questions( out, './dataset/q2.txt')\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载nodes和relations文件，对问句中的人名、地名、机构名、名词做替换，每个问句随机替换10次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "18753 15002 1875\n"
    }
   ],
   "source": [
    "#import pandas as pd\n",
    "import jieba.posseg as pg\n",
    "import csv\n",
    "import random\n",
    "\n",
    "entity_file={'nr':'人名.csv','ns':'地区.csv','nt':'机构.csv','nz':'医学.csv'}\n",
    "root_path = './dataset/'\n",
    "entity_dict = dict()\n",
    "\n",
    "def load_stopwords( stopfile ):\n",
    "    stopword_dict = dict()\n",
    "    with open(stopfile,'r',encoding='utf8') as f:\n",
    "        lines = f.readlines()\n",
    "        for l in lines:\n",
    "            l = l.replace('\\n','')\n",
    "            stopword_dict[l] = 1\n",
    "    return stopword_dict\n",
    "\n",
    "def load_entity():\n",
    "    for k, efile in entity_file.items():\n",
    "        entity_dict[k] = list()\n",
    "        with open(root_path+efile,'r',encoding='utf8') as f:\n",
    "            lines = f.readlines()\n",
    "            for l in lines:\n",
    "                l = l.split(',')\n",
    "                entity_dict[k].append(l[0])\n",
    "\n",
    "def change_entity( id, numbers ): #随机读取n个数字\n",
    "    entity=list()\n",
    "    for i in range(numbers):\n",
    "        index = random.randint(0,len(entity_dict[id])-1)\n",
    "        entity.append(entity_dict[id][index])\n",
    "    return entity\n",
    "\n",
    "def check_jieba():\n",
    "    with open('./dataset/q3.txt','r',encoding='utf8') as f:\n",
    "        csvfile = csv.reader(f)\n",
    "        lines = list()\n",
    "        for q in csvfile:\n",
    "            wordpos = pg.cut(q[0])\n",
    "            line = ''\n",
    "            for w in wordpos:\n",
    "                lines += w.word + '(' + w.flag + ') '\n",
    "            line += '\\n'\n",
    "            lines.append(line)\n",
    "    with open('./dataset/check.txt','w',encoding='utf8') as f:\n",
    "        f.writelines(lines)\n",
    "\n",
    "def data_augment( stopword_dict ):\n",
    "    expand_questions = dict()\n",
    "    with open('./data/q3.txt','r',encoding='utf8') as f:\n",
    "        csvfile = csv.reader(f)\n",
    "        for q in csvfile:\n",
    "            if len(q)==2:\n",
    "                expand_questions[q[0]] = q[1] #先增加原句\n",
    "                wordpos = pg.cut(q[0])\n",
    "                expand = None\n",
    "                wordlist=list()\n",
    "                for w in wordpos: #我 r,爱 v,北京 ns,天安门 ns，钟南山 ns\n",
    "                    wordlist.append(w)\n",
    "                \n",
    "                if len(wordlist)>6:\n",
    "                    continue\n",
    "\n",
    "                for i in range(len(wordlist)):\n",
    "                    if wordlist[i].word in stopword_dict:\n",
    "                        continue\n",
    "                    if wordlist[i].flag in entity_file:\n",
    "                        changes = change_entity(wordlist[i].flag, 200)\n",
    "                        for c in changes:\n",
    "                            ques = ''\n",
    "                            for j in range(len(wordlist)):\n",
    "                                if i==j:\n",
    "                                    ques += c\n",
    "                                else:\n",
    "                                    ques += wordlist[j].word\n",
    "                            if len(ques)<=20:\n",
    "                                expand_questions[ques]=q[1]\n",
    "\n",
    "    questions = list()\n",
    "    for k,v in expand_questions.items():\n",
    "        line = k+','+v+'\\n'\n",
    "        questions.append(line)\n",
    "    random.shuffle(questions) #随机乱序\n",
    "    \n",
    "    n = len(questions)\n",
    "    ntrain = int(n*0.8)\n",
    "    nval = int(n*0.1)\n",
    "    with open('./data/train.txt','w',encoding='utf8') as f:\n",
    "        f.writelines(questions[:ntrain])\n",
    "    with open('./data/val.txt','w',encoding='utf8') as f:\n",
    "        f.writelines(questions[ntrain:ntrain+nval])\n",
    "    with open('./data/test.txt','w',encoding='utf8') as f:\n",
    "        f.writelines(questions[ntrain+nval:])\n",
    "    print(n,ntrain,nval)\n",
    "\n",
    "stopword_dict = load_stopwords('./dataset/stopwords.txt')\n",
    "load_entity()\n",
    "#check_jieba()\n",
    "data_augment(stopword_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8-candidate"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}